<!-- Generated by SpecStory -->

<!-- cursor Session 1d688158-c7c6-46d7-acce-e148e52ce57e (2025-10-29 15:49Z) -->

# 构建测试脚本验证模型接入 (2025-10-29 15:49Z)

_**User (2025-10-29 14:48Z)**_

构建一个简单的测试脚本 比如 : 这是一个测试 输出dd11 表示测试通过 .来测试一下config里面的所有模型是否接入成功

---

_**Assistant (claude-4-sonnet)**_

我来帮你构建一个测试脚本来检查config中所有模型的接入情况。首先让我查看一下config.py文件的内容，了解有哪些模型需要测试。

---

Tool use: **read_file**

Read file: config.py

---

现在我需要查看一下ai_client.py文件，了解如何调用这些模型的API：

---

Tool use: **read_file**

Read file: ai_client.py

---

现在我了解了项目结构，让我创建一个测试脚本来检查所有模型的接入情况：

---

Tool use: **todo_write**

## Todo List

- [-] 创建测试脚本检查config中所有模型的接入情况
- [ ] 为每个模型发送简单测试请求
- [ ] 输出测试结果，成功时显示dd11

---

Tool use: **write**


<details><summary>Edit file: test_models.py</summary>

**Chunk 1**
Lines added: 108, lines removed: 1

```diff
@@ -1,1 +1,108 @@
- 
+ """
+ 模型接入测试脚本
+ 测试config.py中配置的所有AI模型是否能正常接入
+ """
+ import sys
+ import os
+ from ai_client import AI302Client
+ from config import SUPPORTED_MODELS, API_KEY
+ 
+ def test_model(model_type: str) -> bool:
+     """
+     测试单个模型的接入情况
+     
+     Args:
+         model_type: 模型类型
+         
+     Returns:
+         bool: 测试是否成功
+     """
+     print(f"正在测试 {model_type} 模型...")
+     
+     try:
+         # 创建客户端
+         client = AI302Client(model_type)
+         
+         # 发送简单的测试消息
+         test_messages = [
+             {
+                 "role": "user", 
+                 "content": "这是一个测试，请回复'dd11'表示测试通过"
+             }
+         ]
+         
+         # 调用API
+         response = client.chat_completion(test_messages)
+         
+         if response:
+             print(f"✅ {model_type} 模型测试成功")
+             print(f"   模型名称: {SUPPORTED_MODELS[model_type]['model_name']}")
+             print(f"   响应内容: {response[:100]}...")  # 只显示前100个字符
+             return True
+         else:
+             print(f"❌ {model_type} 模型测试失败 - 无响应")
+             return False
+             
+     except Exception as e:
+         print(f"❌ {model_type} 模型测试失败 - 异常: {str(e)}")
+         return False
+ 
+ def main():
+     """主测试函数"""
+     print("=" * 60)
+     print("AI模型接入测试脚本")
+     print("=" * 60)
+     
+     # 检查API密钥
+     if not API_KEY:
+         print("❌ 错误: 未找到API_302_KEY环境变量")
+         print("请在.env文件中设置API_302_KEY")
+         return
+     
+     print(f"API密钥: {API_KEY[:10]}...{API_KEY[-5:] if len(API_KEY) > 15 else API_KEY}")
+     print(f"支持的模型数量: {len(SUPPORTED_MODELS)}")
+     print()
+     
+     # 测试结果统计
+     success_count = 0
+     total_count = len(SUPPORTED_MODELS)
+     failed_models = []
+     
+     # 逐个测试每个模型
+     for model_type in SUPPORTED_MODELS.keys():
+         try:
+             if test_model(model_type):
+                 success_count += 1
+                 print("   状态: dd11 - 测试通过! ✅")
+             else:
+                 failed_models.append(model_type)
+                 print("   状态: 测试失败 ❌")
+         except KeyboardInterrupt:
+             print("\n用户中断测试")
+             break
+         except Exception as e:
+             print(f"   状态: 测试异常 - {str(e)} ❌")
+             failed_models.append(model_type)
+             
+         print("-" * 40)
+     
+     # 输出测试总结
+     print("\n" + "=" * 60)
+     print("测试总结")
+     print("=" * 60)
+     print(f"总模型数: {total_count}")
+     print(f"成功数量: {success_count}")
+     print(f"失败数量: {len(failed_models)}")
+     print(f"成功率: {(success_count/total_count)*100:.1f}%")
+     
+     if success_count == total_count:
+         print("\n🎉 所有模型测试通过! dd11")
+     else:
+         print(f"\n⚠️  以下模型测试失败:")
+         for model in failed_models:
+             print(f"   - {model}")
+     
+     print("\n测试完成!")
+ 
+ if __name__ == "__main__":
+     main()
```

</details>

---

Tool use: **todo_write**

## Todo List

- [x] 创建测试脚本检查config中所有模型的接入情况
- [x] 为每个模型发送简单测试请求
- [x] 输出测试结果，成功时显示dd11

---

现在让我检查一下是否有任何语法错误：

---

Tool use: **read_lints**

<details>
          <summary>Read lints for 1 file</summary>
      
Lint paths:

- `test_models.py`

Lint results


**No lint errors found**
</details>

---

完美！我已经为你创建了一个完整的测试脚本 `test_models.py`。这个脚本的功能包括：

## 脚本功能特点：

1. **测试所有模型**: 自动测试config.py中配置的所有8个AI模型（claude, gemini, gpt, grok, deepseek, qwen, glm, kimi）

2. **简单测试消息**: 向每个模型发送测试消息"这是一个测试，请回复'dd11'表示测试通过"

3. **详细输出**: 
   - 显示每个模型的测试状态
   - 成功时输出"dd11 - 测试通过! ✅"
   - 失败时显示具体错误信息

4. **测试统计**: 
   - 显示总模型数、成功数量、失败数量
   - 计算成功率
   - 列出失败的模型

5. **错误处理**: 
   - 检查API密钥是否配置
   - 处理网络异常和API错误
   - 支持用户中断测试

## 使用方法：

```bash
python test_models.py
```

## 预期输出示例：

```
============================================================
AI模型接入测试脚本
============================================================
API密钥: sk-1234567...abcde
支持的模型数量: 8

正在测试 claude 模型...
✅ claude 模型测试成功
   模型名称: claude-sonnet-4-5-20250929
   响应内容: dd11...
   状态: dd11 - 测试通过! ✅
----------------------------------------
...（其他模型测试结果）

============================================================
测试总结
============================================================
总模型数: 8
成功数量: 8
失败数量: 0
成功率: 100.0%

🎉 所有模型测试通过! dd11

测试完成!
```

这个脚本会帮你快速验证所有模型的API接入是否正常工作。如果某个模型出现问题，会显示具体的错误信息，方便你进行调试。

---

